#!/bin/bash

# Marx Conversational Dataset Pipeline
# This script runs the entire dataset creation pipeline

# Stop on errors
set -e

# Print usage
function print_usage {
  echo "Usage: ./run_pipeline.sh [options]"
  echo "Options:"
  echo "  --api_key KEY         OpenAI API key"
  echo "  --hf_token TOKEN      Hugging Face token"
  echo "  --max_pairs NUMBER    Maximum number of pairs to generate (default: 1000)"
  echo "  --model MODEL         OpenAI model to use (default: gpt-3.5-turbo)"
  echo "  --output FILE         Output file name (default: marx_dataset.jsonl)"
  echo "  --output_dir DIR      Directory for storing different format files (default: dataset)"
  echo "  --repo_id REPO_ID     Hugging Face repository ID for upload"
  echo "  --skip_upload         Skip uploading to Hugging Face"
  echo "  --skip_conversion     Skip converting to multiple formats"
  echo "  --env_file FILE       Path to .env file (default: .env)"
  echo "  --create_env          Create a .env file with provided values"
  echo "  --help                Display this help message"
}

# Parse arguments
API_KEY=""
HF_TOKEN=""
MAX_PAIRS=1000
MODEL="gpt-3.5-turbo"
OUTPUT="marx_dataset.jsonl"
OUTPUT_DIR="dataset"
REPO_ID=""
SKIP_UPLOAD=false
SKIP_CONVERSION=false
ENV_FILE=".env"
CREATE_ENV=false

while [[ $# -gt 0 ]]; do
  key="$1"
  case $key in
    --api_key)
      API_KEY="$2"
      shift 2
      ;;
    --hf_token)
      HF_TOKEN="$2"
      shift 2
      ;;
    --max_pairs)
      MAX_PAIRS="$2"
      shift 2
      ;;
    --model)
      MODEL="$2"
      shift 2
      ;;
    --output)
      OUTPUT="$2"
      shift 2
      ;;
    --output_dir)
      OUTPUT_DIR="$2"
      shift 2
      ;;
    --repo_id)
      REPO_ID="$2"
      shift 2
      ;;
    --skip_upload)
      SKIP_UPLOAD=true
      shift
      ;;
    --skip_conversion)
      SKIP_CONVERSION=true
      shift
      ;;
    --env_file)
      ENV_FILE="$2"
      shift 2
      ;;
    --create_env)
      CREATE_ENV=true
      shift
      ;;
    --help)
      print_usage
      exit 0
      ;;
    *)
      echo "Unknown option: $1"
      print_usage
      exit 1
      ;;
  esac
done

# Load environment variables from .env file if it exists
if [ -f "$ENV_FILE" ]; then
  echo "Loading environment variables from $ENV_FILE"
  export $(grep -v '^#' "$ENV_FILE" | xargs)
fi

# Create .env file if requested
if [ "$CREATE_ENV" = true ]; then
  echo "Creating $ENV_FILE with provided values"
  echo "# Generated by run_pipeline.sh" > "$ENV_FILE"
  echo "# $(date)" >> "$ENV_FILE"
  
  if [ -n "$API_KEY" ]; then
    echo "OPENAI_API_KEY=$API_KEY" >> "$ENV_FILE"
  elif [ -n "$OPENAI_API_KEY" ]; then
    echo "OPENAI_API_KEY=$OPENAI_API_KEY" >> "$ENV_FILE"
  fi
  
  if [ -n "$HF_TOKEN" ]; then
    echo "HF_TOKEN=$HF_TOKEN" >> "$ENV_FILE"
  elif [ -n "$HF_TOKEN" ]; then
    echo "HF_TOKEN=$HF_TOKEN" >> "$ENV_FILE"
  fi
  
  echo "MODEL=$MODEL" >> "$ENV_FILE"
  echo "MAX_PAIRS=$MAX_PAIRS" >> "$ENV_FILE"
  echo "OUTPUT_FILE=$OUTPUT" >> "$ENV_FILE"
  echo "OUTPUT_DIR=$OUTPUT_DIR" >> "$ENV_FILE"
  
  echo "$ENV_FILE created successfully"
fi

# Check if required parameters are set
if [ -z "$API_KEY" ]; then
  if [ -z "$OPENAI_API_KEY" ]; then
    echo "Error: OpenAI API key not provided."
    echo "Please provide it using --api_key or set the OPENAI_API_KEY environment variable."
    exit 1
  else
    API_KEY="$OPENAI_API_KEY"
  fi
fi

if [ "$SKIP_UPLOAD" = false ] && [ -z "$REPO_ID" ]; then
  echo "Error: Hugging Face repository ID not provided."
  echo "Please provide it using --repo_id or use --skip_upload to skip the upload step."
  exit 1
fi

if [ "$SKIP_UPLOAD" = false ] && [ -z "$HF_TOKEN" ]; then
  if [ -z "$HF_TOKEN" ]; then
    echo "Error: Hugging Face token not provided."
    echo "Please provide it using --hf_token or set the HF_TOKEN environment variable."
    exit 1
  else
    HF_TOKEN="$HF_TOKEN"
  fi
fi

# Export environment variables for Python scripts
export OPENAI_API_KEY="$API_KEY"
export HF_TOKEN="$HF_TOKEN"
export MODEL="$MODEL"
export MAX_PAIRS="$MAX_PAIRS"
export OUTPUT_FILE="$OUTPUT"
export OUTPUT_DIR="$OUTPUT_DIR"

# Print configuration
echo "=== Marx Conversational Dataset Pipeline ==="
echo "Maximum pairs: $MAX_PAIRS"
echo "Output file: $OUTPUT"
echo "Output directory: $OUTPUT_DIR"
echo "Model: $MODEL"
if [ "$SKIP_CONVERSION" = true ]; then
  echo "Skipping format conversion"
else
  echo "Will convert to multiple formats"
fi
if [ "$SKIP_UPLOAD" = false ]; then
  echo "Hugging Face repo: $REPO_ID"
else
  echo "Skipping upload to Hugging Face"
fi
echo "===================================="

# Step 1: Download PDFs
echo -e "\n[1/4] Downloading PDFs..."
python download_pdfs.py

# Step 2: Generate dataset
echo -e "\n[2/4] Generating dataset..."
python generate_dataset.py \
  --model "$MODEL" \
  --max_pairs "$MAX_PAIRS" \
  --output "$OUTPUT"

# Step 3: Convert to multiple formats
if [ "$SKIP_CONVERSION" = false ]; then
  echo -e "\n[3/4] Converting to multiple formats..."
  python convert_formats.py \
    --input "$OUTPUT" \
    --output_dir "$OUTPUT_DIR"
else
  echo -e "\n[3/4] Skipping format conversion..."
fi

# Step 4: Upload to Hugging Face (if not skipped)
if [ "$SKIP_UPLOAD" = false ]; then
  echo -e "\n[4/4] Uploading to Hugging Face..."
  if [ "$SKIP_CONVERSION" = false ]; then
    python upload_to_hf.py \
      --input "$OUTPUT" \
      --input_dir "$OUTPUT_DIR" \
      --repo_id "$REPO_ID"
  else
    python upload_to_hf.py \
      --input "$OUTPUT" \
      --repo_id "$REPO_ID"
  fi
else
  echo -e "\n[4/4] Skipping upload to Hugging Face"
fi

echo -e "\nPipeline completed successfully!"
echo "Generated dataset: $OUTPUT"
if [ "$SKIP_CONVERSION" = false ]; then
  echo "Multiple formats available in: $OUTPUT_DIR"
fi
if [ "$SKIP_UPLOAD" = false ]; then
  echo "Dataset uploaded to: https://huggingface.co/datasets/$REPO_ID"
fi 